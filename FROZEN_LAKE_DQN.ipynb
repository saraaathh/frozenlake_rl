{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OAIQFKNKmOb0",
        "outputId": "481040d5-0885-4402-c90e-ae8a7b44f3af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym) (0.0.8)\n"
          ]
        }
      ],
      "source": [
        "pip install gym"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import random"
      ],
      "metadata": {
        "id": "nG9kgzMUmPHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_states, h1_nodes, out_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define network layers\n",
        "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
        "        self.out = nn.Linear(h1_nodes, out_actions) # ouptut layer w\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
        "        x = self.out(x)         # Calculate output\n",
        "        return x\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# FrozeLake Deep Q-Learning\n",
        "class FrozenLakeDQL():\n",
        "    # Hyperparameters (adjustable)\n",
        "    learning_rate_a = 0.001         # learning rate (alpha)\n",
        "    discount_factor_g = 0.9         # discount rate (gamma)\n",
        "    network_sync_rate = 10          # number of steps the agent takes before syncing the policy and target network\n",
        "    replay_memory_size = 1000       # size of replay memory\n",
        "    mini_batch_size = 32            # size of the training data set sampled from the replay memory\n",
        "\n",
        "    # Neural Network\n",
        "    loss_fn = nn.MSELoss()          # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
        "    optimizer = None                # NN Optimizer. Initialize later.\n",
        "\n",
        "    ACTIONS = ['L','D','R','U']     # for printing 0,1,2,3 => L(eft),D(own),R(ight),U(p)\n",
        "\n",
        "    # Train the FrozeLake environment\n",
        "    def train(self, episodes, render=False, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
        "        num_states = env.observation_space.n\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        epsilon = 1 # 1 = 100% random actions\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "        target_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        print('Policy (random, before training):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count=0\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset() # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False       # True when agent takes more than 200 actions\n",
        "\n",
        "            # Agent navigates map until it falls into hole/reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action\n",
        "                    action = env.action_space.sample() # actions: 0=left,1=down,2=right,3=up\n",
        "                else:\n",
        "                    # select best action\n",
        "                    with torch.no_grad():\n",
        "                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "\n",
        "                # Execute action\n",
        "                new_state,reward,terminated,truncated = env.step(action)\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action, new_state, reward, terminated))\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count+=1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            if reward == 1:\n",
        "                rewards_per_episode[i] = 1\n",
        "\n",
        "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
        "            if len(memory)>self.mini_batch_size and np.sum(rewards_per_episode)>0:\n",
        "                mini_batch = memory.sample(self.mini_batch_size)\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                # Decay epsilon\n",
        "                epsilon = max(epsilon - 1/episodes, 0)\n",
        "                epsilon_history.append(epsilon)\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count=0\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "\n",
        "        # Save policy\n",
        "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
        "\n",
        "        # Create new graph\n",
        "        plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        sum_rewards = np.zeros(episodes)\n",
        "        for x in range(episodes):\n",
        "            sum_rewards[x] = np.sum(rewards_per_episode[max(0, x-100):(x+1)])\n",
        "        plt.subplot(121) # plot on a 1 row x 2 col grid, at cell 1\n",
        "        plt.plot(sum_rewards)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        plt.subplot(122) # plot on a 1 row x 2 col grid, at cell 2\n",
        "        plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        plt.savefig('frozen_lake_dql.png')\n",
        "\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        # Get number of input nodes\n",
        "        num_states = policy_dqn.fc1.in_features\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action, new_state, reward, terminated in mini_batch:\n",
        "\n",
        "            if terminated:\n",
        "                # Agent either reached goal (reward=1) or fell into hole (reward=0)\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            # Adjust the specific action to the target that was just calculated\n",
        "            target_q[action] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts an state (int) to a tensor representation.\n",
        "    For example, the FrozenLake 4x4 map has 4x4=16 states numbered from 0 to 15.\n",
        "\n",
        "    Parameters: state=1, num_states=16\n",
        "    Return: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state:int, num_states:int)->torch.Tensor:\n",
        "        input_tensor = torch.zeros(num_states)\n",
        "        input_tensor[state] = 1\n",
        "        return input_tensor\n",
        "\n",
        "    # Run the FrozeLake environment with the learned policy\n",
        "    def test(self, episodes, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human')\n",
        "        num_states = env.observation_space.n\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=num_states, out_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
        "        policy_dqn.eval()    # switch model to evaluation mode\n",
        "\n",
        "        print('Policy (trained):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()  # Initialize to state 0\n",
        "            terminated = False      # True when agent falls in hole or reached goal\n",
        "            truncated = False       # True when agent takes more than 200 actions\n",
        "\n",
        "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while(not terminated and not truncated):\n",
        "                # Select best action\n",
        "                with torch.no_grad():\n",
        "                    action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "\n",
        "                # Execute action\n",
        "                state,reward,terminated,truncated = env.step(action)\n",
        "\n",
        "        env.close()\n",
        "\n",
        "    # Print DQN: state, best action, q values\n",
        "    def print_dqn(self, dqn):\n",
        "        # Get number of input nodes\n",
        "        num_states = dqn.fc1.in_features\n",
        "\n",
        "        # Loop each state and print policy to console\n",
        "        for s in range(num_states):\n",
        "            #  Format q values for printing\n",
        "            q_values = ''\n",
        "            for q in dqn(self.state_to_dqn_input(s, num_states)).tolist():\n",
        "                q_values += \"{:+.2f}\".format(q)+' '  # Concatenate q values, format to 2 decimals\n",
        "            q_values=q_values.rstrip()              # Remove space at the end\n",
        "\n",
        "            # Map the best action to L D R U\n",
        "            best_action = self.ACTIONS[dqn(self.state_to_dqn_input(s, num_states)).argmax()]\n",
        "\n",
        "            # Print policy in the format of: state, action, q values\n",
        "            # The printed layout matches the FrozenLake map.\n",
        "            print(f'{s:02},{best_action},[{q_values}]', end=' ')\n",
        "            if (s+1)%4==0:\n",
        "                print() # Print a newline every 4 states\n",
        "\n"
      ],
      "metadata": {
        "id": "GW4b4Z5MmUq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "\n",
        "    frozen_lake = FrozenLakeDQL()\n",
        "    is_slippery = False\n",
        "    frozen_lake.train(1000, is_slippery=is_slippery)\n",
        "    frozen_lake.test(10, is_slippery=is_slippery)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "YpCbCz3p7MJ6",
        "outputId": "d3a8ac9f-0fc0-4a59-ad0f-c3fee9b0bb51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy (random, before training):\n",
            "00,D,[-0.20 +0.12 -0.08 +0.09] 01,R,[-0.23 +0.13 +0.18 +0.06] 02,D,[-0.15 +0.09 +0.07 +0.05] 03,D,[-0.19 +0.09 +0.02 +0.08] \n",
            "04,R,[-0.10 +0.04 +0.08 -0.07] 05,R,[-0.21 +0.10 +0.18 +0.10] 06,R,[-0.12 +0.08 +0.11 +0.01] 07,D,[-0.17 +0.10 +0.00 +0.02] \n",
            "08,R,[-0.21 +0.08 +0.14 +0.05] 09,D,[-0.07 +0.08 +0.04 -0.04] 10,D,[-0.07 +0.11 +0.04 -0.00] 11,D,[-0.19 +0.14 +0.07 +0.03] \n",
            "12,D,[-0.21 +0.07 -0.04 +0.02] 13,U,[-0.30 +0.04 +0.09 +0.20] 14,U,[-0.20 -0.01 +0.04 +0.05] 15,R,[-0.23 +0.05 +0.12 +0.08] \n",
            "Policy (trained):\n",
            "00,D,[-0.20 +0.12 -0.08 +0.09] 01,R,[-0.23 +0.13 +0.18 +0.06] 02,D,[-0.15 +0.09 +0.07 +0.05] 03,D,[-0.19 +0.09 +0.02 +0.08] \n",
            "04,R,[-0.10 +0.04 +0.08 -0.07] 05,R,[-0.21 +0.10 +0.18 +0.10] 06,R,[-0.12 +0.08 +0.11 +0.01] 07,D,[-0.17 +0.10 +0.00 +0.02] \n",
            "08,R,[-0.21 +0.08 +0.14 +0.05] 09,D,[-0.07 +0.08 +0.04 -0.04] 10,D,[-0.07 +0.11 +0.04 -0.00] 11,D,[-0.19 +0.14 +0.07 +0.03] \n",
            "12,D,[-0.21 +0.07 -0.04 +0.02] 13,U,[-0.30 +0.04 +0.09 +0.20] 14,U,[-0.20 -0.01 +0.04 +0.05] 15,R,[-0.23 +0.05 +0.12 +0.08] \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import  gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import random\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Define model\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, in_states, h1_nodes, out_actions):\n",
        "        super().__init__()\n",
        "\n",
        "        # Define network layers\n",
        "        self.fc1 = nn.Linear(in_states, h1_nodes)   # first fully connected layer\n",
        "        self.out = nn.Linear(h1_nodes, out_actions) # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x)) # Apply rectified linear unit (ReLU) activation\n",
        "        x = self.out(x)         # Calculate output\n",
        "        return x\n",
        "\n",
        "# Define memory for Experience Replay\n",
        "class ReplayMemory():\n",
        "    def __init__(self, maxlen):\n",
        "        self.memory = deque([], maxlen=maxlen)\n",
        "\n",
        "    def append(self, transition):\n",
        "        self.memory.append(transition)\n",
        "\n",
        "    def sample(self, sample_size):\n",
        "        return random.sample(self.memory, sample_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)\n",
        "\n",
        "# FrozenLake Deep Q-Learning\n",
        "class FrozenLakeDQL():\n",
        "    # Hyperparameters (adjustable)\n",
        "    learning_rate_a = 0.0005         # learning rate (alpha)\n",
        "    discount_factor_g = 0.99         # discount rate (gamma)\n",
        "    network_sync_rate = 5            # number of steps before syncing the policy and target network\n",
        "    replay_memory_size = 5000        # size of replay memory\n",
        "    mini_batch_size = 64             # size of the training data set sampled from the replay memory\n",
        "\n",
        "    # Neural Network\n",
        "    loss_fn = nn.MSELoss()           # NN Loss function. MSE=Mean Squared Error can be swapped to something else.\n",
        "    optimizer = None                 # NN Optimizer. Initialize later.\n",
        "\n",
        "    ACTIONS = ['L','D','R','U']      # for printing 0,1,2,3 => L(eft),D(own),R(ight),U(p)\n",
        "\n",
        "    # Train the FrozenLake environment\n",
        "    def train(self, episodes, render=False, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human' if render else None)\n",
        "        num_states = env.observation_space.n\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        epsilon = 1  # Initial epsilon value for exploration\n",
        "        epsilon_decay = 0.995  # Epsilon decay rate\n",
        "        memory = ReplayMemory(self.replay_memory_size)\n",
        "\n",
        "        # Create policy and target network. Number of nodes in the hidden layer can be adjusted.\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=64, out_actions=num_actions)\n",
        "        target_dqn = DQN(in_states=num_states, h1_nodes=64, out_actions=num_actions)\n",
        "\n",
        "        # Make the target and policy networks the same (copy weights/biases from one network to the other)\n",
        "        target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "\n",
        "        print('Policy (random, before training):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        # Policy network optimizer. \"Adam\" optimizer can be swapped to something else.\n",
        "        self.optimizer = torch.optim.Adam(policy_dqn.parameters(), lr=self.learning_rate_a)\n",
        "\n",
        "        # List to keep track of rewards collected per episode. Initialize list to 0's.\n",
        "        rewards_per_episode = np.zeros(episodes)\n",
        "\n",
        "        # List to keep track of epsilon decay\n",
        "        epsilon_history = []\n",
        "\n",
        "        # Track number of steps taken. Used for syncing policy => target network.\n",
        "        step_count = 0\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()  # Initialize to state 0\n",
        "            terminated = False   # True when agent falls in hole or reached goal\n",
        "            truncated = False    # True when agent takes more than 200 actions\n",
        "\n",
        "            # Agent navigates map until it falls into a hole/reaches the goal (terminated), or has taken 200 actions (truncated).\n",
        "            while not terminated and not truncated:\n",
        "\n",
        "                # Select action based on epsilon-greedy\n",
        "                if random.random() < epsilon:\n",
        "                    # select random action\n",
        "                    action = env.action_space.sample()  # actions: 0=left,1=down,2=right,3=up\n",
        "                else:\n",
        "                    # select best action\n",
        "                    with torch.no_grad():\n",
        "                        action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "\n",
        "                # Execute action\n",
        "                new_state, reward, terminated, truncated = env.step(action)\n",
        "\n",
        "                # Save experience into memory\n",
        "                memory.append((state, action, new_state, reward, terminated))\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                # Increment step counter\n",
        "                step_count += 1\n",
        "\n",
        "            # Keep track of the rewards collected per episode.\n",
        "            if reward == 1:\n",
        "                rewards_per_episode[i] = 1\n",
        "\n",
        "            # Check if enough experience has been collected and if at least 1 reward has been collected\n",
        "            if len(memory) > self.mini_batch_size and np.sum(rewards_per_episode) > 0:\n",
        "                mini_batch = memory.sample(self.mini_batch_size)\n",
        "                self.optimize(mini_batch, policy_dqn, target_dqn)\n",
        "\n",
        "                # Decay epsilon\n",
        "                epsilon = max(epsilon * epsilon_decay, 0.01)\n",
        "                epsilon_history.append(epsilon)\n",
        "\n",
        "                # Copy policy network to target network after a certain number of steps\n",
        "                if step_count > self.network_sync_rate:\n",
        "                    target_dqn.load_state_dict(policy_dqn.state_dict())\n",
        "                    step_count = 0\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "\n",
        "        # Save policy\n",
        "        torch.save(policy_dqn.state_dict(), \"frozen_lake_dql.pt\")\n",
        "\n",
        "        # Create new graph\n",
        "        plt.figure(1)\n",
        "\n",
        "        # Plot average rewards (Y-axis) vs episodes (X-axis)\n",
        "        sum_rewards = np.zeros(episodes)\n",
        "        for x in range(episodes):\n",
        "            sum_rewards[x] = np.sum(rewards_per_episode[max(0, x-100):(x+1)])\n",
        "        plt.subplot(121)  # plot on a 1 row x 2 col grid, at cell 1\n",
        "        plt.plot(sum_rewards)\n",
        "\n",
        "        # Plot epsilon decay (Y-axis) vs episodes (X-axis)\n",
        "        plt.subplot(122)  # plot on a 1 row x 2 col grid, at cell 2\n",
        "        plt.plot(epsilon_history)\n",
        "\n",
        "        # Save plots\n",
        "        plt.savefig('frozen_lake_dql.png')\n",
        "\n",
        "    # Optimize policy network\n",
        "    def optimize(self, mini_batch, policy_dqn, target_dqn):\n",
        "\n",
        "        # Get number of input nodes\n",
        "        num_states = policy_dqn.fc1.in_features\n",
        "\n",
        "        current_q_list = []\n",
        "        target_q_list = []\n",
        "\n",
        "        for state, action, new_state, reward, terminated in mini_batch:\n",
        "\n",
        "            if terminated:\n",
        "                # Agent either reached the goal (reward=1) or fell into a hole (reward=0)\n",
        "                # When in a terminated state, target q value should be set to the reward.\n",
        "                target = torch.FloatTensor([reward])\n",
        "            else:\n",
        "                # Calculate target q value\n",
        "                with torch.no_grad():\n",
        "                    target = torch.FloatTensor(\n",
        "                        reward + self.discount_factor_g * target_dqn(self.state_to_dqn_input(new_state, num_states)).max()\n",
        "                    )\n",
        "\n",
        "            # Get the current set of Q values\n",
        "            current_q = policy_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            current_q_list.append(current_q)\n",
        "\n",
        "            # Get the target set of Q values\n",
        "            target_q = target_dqn(self.state_to_dqn_input(state, num_states))\n",
        "            # Adjust the specific action to the target that was just calculated\n",
        "            target_q[action] = target\n",
        "            target_q_list.append(target_q)\n",
        "\n",
        "        # Compute loss for the whole minibatch\n",
        "        loss = self.loss_fn(torch.stack(current_q_list), torch.stack(target_q_list))\n",
        "\n",
        "        # Optimize the model\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "    '''\n",
        "    Converts a state (int) to a tensor representation.\n",
        "    For example, the FrozenLake 4x4 map has 4x4=16 states numbered from 0 to 15.\n",
        "\n",
        "    Parameters: state=1, num_states=16\n",
        "    Return: tensor([0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
        "    '''\n",
        "    def state_to_dqn_input(self, state:int, num_states:int) -> torch.Tensor:\n",
        "        input_tensor = torch.zeros(num_states)\n",
        "        input_tensor[state] = 1\n",
        "        return input_tensor\n",
        "\n",
        "    # Run the FrozenLake environment with the learned policy\n",
        "    def test(self, episodes, is_slippery=False):\n",
        "        # Create FrozenLake instance\n",
        "        env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=is_slippery, render_mode='human')\n",
        "        num_states = env.observation_space.n\n",
        "        num_actions = env.action_space.n\n",
        "\n",
        "        # Load learned policy\n",
        "        policy_dqn = DQN(in_states=num_states, h1_nodes=64, out_actions=num_actions)\n",
        "        policy_dqn.load_state_dict(torch.load(\"frozen_lake_dql.pt\"))\n",
        "        policy_dqn.eval()  # Switch model to evaluation mode\n",
        "\n",
        "        print('Policy (learned, after training):')\n",
        "        self.print_dqn(policy_dqn)\n",
        "\n",
        "        for i in range(episodes):\n",
        "            state = env.reset()  # Initialize to state 0\n",
        "            terminated = False\n",
        "            truncated = False\n",
        "            print(f\"\\nEPISODE {i+1}\\n--------\")\n",
        "\n",
        "            # Agent navigates map until it falls into a hole (terminated), reaches goal (terminated), or has taken 200 actions (truncated).\n",
        "            while not terminated and not truncated:\n",
        "\n",
        "                # Select best action\n",
        "                action = policy_dqn(self.state_to_dqn_input(state, num_states)).argmax().item()\n",
        "                new_state, reward, terminated, truncated = env.step(action)\n",
        "\n",
        "                # Move to the next state\n",
        "                state = new_state\n",
        "\n",
        "                print(f\"Action: {self.ACTIONS[action]} | Reward: {reward}\")\n",
        "\n",
        "        # Close environment\n",
        "        env.close()\n",
        "\n",
        "    # Print the Q-values from the DQN for each state in the FrozenLake map.\n",
        "    def print_dqn(self, dqn):\n",
        "        num_states = dqn.fc1.in_features\n",
        "\n",
        "        for state in range(num_states):\n",
        "            q_values = dqn(self.state_to_dqn_input(state, num_states))\n",
        "            print(f'{state}: L={q_values[0]:.3f}, D={q_values[1]:.3f}, R={q_values[2]:.3f}, U={q_values[3]:.3f}')\n",
        "\n",
        "# Test model\n",
        "dql = FrozenLakeDQL()\n",
        "dql.train(episodes=1000)\n",
        "dql.test(episodes=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ch-371Pa9qvq",
        "outputId": "0a1d7ee2-3687-482b-e464-9bd1183fa3b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy (random, before training):\n",
            "0: L=0.014, D=0.032, R=0.035, U=0.071\n",
            "1: L=0.050, D=0.092, R=0.117, U=-0.013\n",
            "2: L=0.088, D=-0.007, R=0.090, U=0.015\n",
            "3: L=0.109, D=0.084, R=0.089, U=0.070\n",
            "4: L=0.041, D=0.080, R=0.068, U=0.024\n",
            "5: L=0.023, D=0.087, R=0.049, U=0.022\n",
            "6: L=0.051, D=-0.012, R=0.109, U=0.007\n",
            "7: L=0.052, D=0.123, R=0.105, U=0.037\n",
            "8: L=0.067, D=0.004, R=0.027, U=0.062\n",
            "9: L=-0.005, D=0.040, R=0.042, U=0.033\n",
            "10: L=0.051, D=0.063, R=0.055, U=-0.002\n",
            "11: L=0.026, D=0.079, R=0.132, U=0.023\n",
            "12: L=0.054, D=0.008, R=0.020, U=0.091\n",
            "13: L=-0.013, D=0.076, R=0.030, U=0.060\n",
            "14: L=0.018, D=0.001, R=0.098, U=0.018\n",
            "15: L=0.092, D=0.034, R=0.131, U=-0.010\n",
            "Policy (learned, after training):\n",
            "0: L=0.014, D=0.032, R=0.035, U=0.071\n",
            "1: L=0.050, D=0.092, R=0.117, U=-0.013\n",
            "2: L=0.088, D=-0.007, R=0.090, U=0.015\n",
            "3: L=0.109, D=0.084, R=0.089, U=0.070\n",
            "4: L=0.041, D=0.080, R=0.068, U=0.024\n",
            "5: L=0.023, D=0.087, R=0.049, U=0.022\n",
            "6: L=0.051, D=-0.012, R=0.109, U=0.007\n",
            "7: L=0.052, D=0.123, R=0.105, U=0.037\n",
            "8: L=0.067, D=0.004, R=0.027, U=0.062\n",
            "9: L=-0.005, D=0.040, R=0.042, U=0.033\n",
            "10: L=0.051, D=0.063, R=0.055, U=-0.002\n",
            "11: L=0.026, D=0.079, R=0.132, U=0.023\n",
            "12: L=0.054, D=0.008, R=0.020, U=0.091\n",
            "13: L=-0.013, D=0.076, R=0.030, U=0.060\n",
            "14: L=0.018, D=0.001, R=0.098, U=0.018\n",
            "15: L=0.092, D=0.034, R=0.131, U=-0.010\n",
            "\n",
            "EPISODE 1\n",
            "--------\n",
            "Action: U | Reward: 0.0\n",
            "\n",
            "EPISODE 2\n",
            "--------\n",
            "Action: U | Reward: 0.0\n",
            "\n",
            "EPISODE 3\n",
            "--------\n",
            "Action: U | Reward: 0.0\n",
            "\n",
            "EPISODE 4\n",
            "--------\n",
            "Action: U | Reward: 0.0\n",
            "\n",
            "EPISODE 5\n",
            "--------\n",
            "Action: U | Reward: 0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}